"""
10 nov. 2014

iterative mapping copied from hiclib

"""

import os
import tempfile
import gzip
import pysam
from warnings import warn
from itertools import combinations
from re import findall, compile as recompile
try:
    import gem
except ImportError:
    warn('WARNING: GEMTOOLS not found')

N_WINDOWS = 0


def eq_reads(rd1, rd2):
    """
    Compare reads accounting for multicontacts
    """
    return rd1.split('~', 1)[0] == rd2.split('~', 1)[0]

def get_intersection(fname1, fname2, out_path, verbose=False):
    """
    Merges the two files corresponding to each reads sides. Reads found in both
       files are merged and written in an output file.

    Dealing with multiple contacts:
       - a pairwise contact is created for each possible combnation of the
         multicontacts. The name of the read is extended by '# 1/3' in case
         the reported pairwise contact corresponds to the first of 3 possibles
       - it may happen that different contacts are mapped on a single RE fragment
         (if each are on different end), in which case:
          - if no other fragment from this read are mapped than, both are kept
          - otherwise, they are merged into one longer (as if they were mapped
            in the positive strand)

    :param fname1: path to a tab separated file generated by the function
       :func:`pytadbit.parsers.sam_parser.parse_sam`
    :param fname2: path to a tab separated file generated by the function
       :func:`pytadbit.parsers.sam_parser.parse_sam`
    :param out_path: path to an outfile. It will written in a similar format as
       the inputs
    """
    # make sure that we are comparing strings in the same way as the bash sort
    # command
    import locale
    try:
        locale.setlocale(locale.LC_ALL, '.'.join(locale.getdefaultlocale()))
    except ValueError:
        locale.setlocale(locale.LC_ALL, '.'.join(('en_US', locale.getdefaultlocale())))

    reads_fh = open(out_path, 'w')
    reads1 = open(fname1)
    line1 = reads1.next()
    header1 = ''
    while line1.startswith('#'):
        if line1.startswith('# CRM'):
            header1 += line1
        line1 = reads1.next()
    read1 = line1.split('\t', 1)[0]

    reads2 = open(fname2)
    line2 = reads2.next()
    header2 = ''
    while line2.startswith('#'):
        if line2.startswith('# CRM'):
            header2 += line2
        line2 = reads2.next()
    read2 = line2.split('\t', 1)[0]
    if header1 != header2:
        raise Exception('seems to be mapped onover different chromosomes\n')
    # setup REGEX to split reads in a single line
    # readex = recompile('((?:[^\t]+\t){6}[^\t]+)')
    # writes header in output
    reads_fh.write(header1)
    # writes common reads
    count = 0
    try:
        while True:
            if eq_reads(read1, read2):
                count += 1
                # case we have potential multicontacts
                if '|||' in line1 or '|||' in line2:
                    elts = {}
                    for read in line1.split('|||'):
                        nam, crm, pos, strd, nts, beg, end = read.strip().split('\t')
                        elts.setdefault((crm, beg, end), []).append(
                            (nam, crm, pos, strd, nts, beg, end))
                    for read in line2.split('|||'):
                        nam, crm, pos, strd, nts, beg, end = read.strip().split('\t')
                        elts.setdefault((crm, beg, end), []).append(
                            (nam, crm, pos, strd, nts, beg, end))
                    # write contacts by pairs
                    for elt in elts:
                        if len(elts[elt]) == 1:
                            elts[elt] = elts[elt][0]
                        elif len(elts) == 1:
                            elts[elt] = sorted(
                                elts[elt],
                                key=lambda x: int(x[2]))[::len(elts[elt])-1]
                            elts1 = {elt: elts[elt][0]}
                            elts2 = {elt: elts[elt][1]}
                        else:
                            map1, map2 = sorted(
                                elts[elt],
                                key=lambda x: int(x[2]))[::len(elts[elt])-1]
                            elts[elt] = map1
                            if map1[3] == '1':
                                beg = int(map1[2])
                            else:
                                beg = int(map1[2]) - int(map1[4]) - 1
                            if map2[3] == '0':
                                nts = int(map2[2]) - beg
                            else:
                                nts = int(map2[2]) + int(map2[4]) + 1 - beg
                            elts[elt] = tuple(list(elts[elt][:2]) +
                                              [str(beg), '1', str(nts)] +
                                              list(elts[elt][5:]))
                    contacts = len(elts) - 1
                    if contacts > 1:
                        for i, (r1, r2) in enumerate(combinations(elts.values(), 2)):
                            r1, r2 = sorted((r1, r2), key=lambda x: x[1:2])
                            reads_fh.write(r1[0] + (
                                '#%d/%d' % (i + 1, contacts * (contacts + 1) / 2)) +
                                           '\t' + '\t'.join(r1[1:]) + '\t' + 
                                           '\t'.join(r2[1:]) + '\n')
                    elif contacts == 1:
                        r1, r2 = sorted((elts.values()[0], elts.values()[1]),
                                        key=lambda x: x[1:2])
                        reads_fh.write('\t'.join(r1) + '\t' + 
                                       '\t'.join(r2[1:]) + '\n')
                    else:
                        r1, r2 = sorted((elts1.values()[0], elts2.values()[0]),
                                        key=lambda x: x[1:2])
                        reads_fh.write('\t'.join(r1) + '\t' + 
                                       '\t'.join(r2[1:]) + '\n')
                else:
                    r1, r2 = sorted((line1.split(), line2.split()),
                                    key=lambda x: x[1:2])
                    reads_fh.write('\t'.join(r1) + '\t' +
                                   '\t'.join(r2[1:]) + '\n')
                line1 = reads1.next()
                read1 = line1.split('\t', 1)[0]
                line2 = reads2.next()
                read2 = line2.split('\t', 1)[0]
            # if first element of line1 is greater than the one of line2:
            elif locale.strcoll(read1, read2) > 0:
                line2 = reads2.next()
                read2 = line2.split('\t', 1)[0]
            else:
                line1 = reads1.next()
                read1 = line1.split('\t', 1)[0]
    except StopIteration:
        reads1.close()
        reads2.close()
    reads_fh.close()
    if verbose:
        print 'Found %d pair of reads mapping uniquely' % count

def trimming(raw_seq_len, seq_start, min_seq_len):
    return seq_start, raw_seq_len - seq_start - min_seq_len

def iterative_mapping(gem_index_path, fastq_path, out_sam_path,
                      range_start, range_stop, **kwargs):
    """
    Map iteratively a given FASTQ file to a reference genome.
    
    :param gem_index_path: path to index file created from a reference genome
       using gem-index tool
    :param fastq_path: PATH to fastq file, either compressed or not.
    :param out_sam_path: path to a directory where to store mapped reads in SAM/
       BAM format (see option output_is_bam).
    :param range_start: list of integers representing the start position of each
       read fragment to be mapped (starting at 1 includes the first nucleotide
       of the read).
    :param range_stop: list of integers representing the end position of each
       read fragment to be mapped.
    :param True single_end: when FASTQ contains paired-ends flags
    :param 4 nthreads: number of threads to use for mapping (number of CPUs)
    :param 0.04 max_edit_distance: The maximum number of edit operations allowed
       while verifying candidate matches by dynamic programming.
    :param 0.04 mismatches: The maximum number of nucleotide substitutions
       allowed while mapping each k-mer. It is always guaranteed that, however
       other options are chosen, all the matches up to the specified number of
       substitutions will be found by the program.
    :param -1 max_reads_per_chunk: maximum number of reads to process at a time.
       If -1, all reads will be processed in one run (more RAM memory needed).
    :param False output_is_bam: Use binary (compressed) form of generated
       out-files with mapped reads (recommended to save disk space).
    :param /tmp temp_dir: important to change. Intermediate FASTQ files will be
       written there.

    :returns: a list of paths to generated outfiles. To be passed to 
       :func:`pytadbit.parsers.sam_parser.parse_sam`
    """
    gem_index_path      = os.path.abspath(os.path.expanduser(gem_index_path))
    fastq_path          = os.path.abspath(os.path.expanduser(fastq_path))
    out_sam_path        = os.path.abspath(os.path.expanduser(out_sam_path))
    single_end          = kwargs.get('single_end'          , True)
    max_edit_distance   = kwargs.get('max_edit_distance'   , 0.04)
    mismatches          = kwargs.get('mismatches'          , 0.04)
    nthreads            = kwargs.get('nthreads'            , 4)
    max_reads_per_chunk = kwargs.get('max_reads_per_chunk' , -1)
    out_files           = kwargs.get('out_files'           , [])
    output_is_bam       = kwargs.get('output_is_bam'       , False)
    temp_dir = os.path.abspath(os.path.expanduser(
        kwargs.get('temp_dir', tempfile.gettempdir())))

    # check kwargs
    for kw in kwargs:
        if not kw in ['single_end', 'nthreads', 'max_edit_distance',
                      'mismatches', 'max_reads_per_chunk',
                      'out_files', 'output_is_bam', 'temp_dir']:
            warn('WARNING: %s not is usual keywords, misspelled?' % kw)
    
    # check windows:
    if not isinstance(range_start, list) or not isinstance(range_stop, list):
        if (not isinstance(range_start, tuple) or
            not isinstance(range_stop, tuple)):
            raise Exception('ERROR: range_start and range_stop should be lists')
        range_start = list(range_start)
        range_stop  = list(range_stop)
    if (not all(isinstance(i, int) for i in range_start) or
        not all(isinstance(i, int) for i in range_stop)):
        try:
            range_start = map(int, range_start)
            range_stop  = map(int, range_stop)            
            warn('WARNING: range_start and range_stop converted to integers')
        except ValueError:
            raise Exception('ERROR: range_start and range_stop should contain' +
                            ' integers only')
    if (len(zip(range_start, range_stop)) < len(range_start) or
        len(range_start) != len(range_stop)):
        raise Exception('ERROR: range_start and range_stop should have the ' +
                        'same sizes and windows should be uniques.')
    if any([i >= j for i, j in zip(range_start, range_stop)]):
        raise Exception('ERROR: start positions should always be lower than ' +
                        'stop positions.')
    if any([i <= 0 for i in range_start]):
        raise Exception('ERROR: start positions should be strictly positive.')

    # create directories
    for rep in [temp_dir, os.path.split(out_sam_path)[0]]:
        try:
            os.mkdir(rep)
        except OSError, error:
            if error.strerror != 'File exists':
                raise error

    #get the length of a read
    if fastq_path.endswith('.gz'):
        fastqh = gzip.open(fastq_path)
    else:
        fastqh = open(fastq_path)
    # get the length from the length of the second line, which is the sequence
    # can not use the "length" keyword, as it is not always present
    try:
        _ = fastqh.next()
        raw_seq_len = len(fastqh.next().strip())
        fastqh.close()
    except StopIteration:
        raise IOError('ERROR: problem reading %s\n' % fastq_path)

    if not  N_WINDOWS:
        N_WINDOWS = len(range_start)
    # Split input files if required and apply iterative mapping to each
    # segment separately.
    if max_reads_per_chunk > 0:
        kwargs['max_reads_per_chunk'] = -1
        print 'Split input file %s into chunks' % fastq_path
        chunked_files = _chunk_file(
            fastq_path,
            os.path.join(temp_dir, os.path.split(fastq_path)[1]),
            max_reads_per_chunk * 4)
        print '%d chunks obtained' % len(chunked_files)
        for i, fastq_chunk_path in enumerate(chunked_files):
            global N_WINDOWS
            N_WINDOWS = 0
            print 'Run iterative_mapping recursively on %s' % fastq_chunk_path
            out_files.extend(iterative_mapping(
                gem_index_path, fastq_chunk_path,
                out_sam_path + '.%d' % (i + 1), range_start[:], range_stop[:],
                **kwargs))

        for i, fastq_chunk_path in enumerate(chunked_files):
            # Delete chunks only if the file was really chunked.
            if len(chunked_files) > 1:
                print 'Remove the chunks: %s' % ' '.join(chunked_files)
                os.remove(fastq_chunk_path)
        return out_files

    # end position according to sequence in the file
    # removes 1 in order to start at 1 instead of 0
    try:
        seq_end = range_stop.pop(0)
        seq_beg = range_start.pop(0)
    except IndexError:
        return out_files

    # define what we trim
    seq_len = seq_end - seq_beg
    trim_5, trim_3 = trimming(raw_seq_len, seq_beg - 1, seq_len - 1)

    # output
    local_out_sam = out_sam_path + '.%d:%d-%d' % (
        N_WINDOWS - len(range_stop), seq_beg, seq_end)
    out_files.append(local_out_sam)
    # input
    inputf = gem.files.open(fastq_path)

    # trimming
    trimmed = gem.filter.run_filter(
        inputf, ['--hard-trim', '%d,%d' % (trim_5, trim_3)],
        threads=nthreads, paired=not single_end)

    # mapping
    mapped = gem.mapper(trimmed, gem_index_path, min_decoded_strata=0,
                        max_decoded_matches=2, unique_mapping=False,
                        max_edit_distance=max_edit_distance,
                        mismatches=mismatches,
                        output=temp_dir + '/test.map',
                        threads=nthreads)

    # convert to sam/bam
    if output_is_bam:
        sam = gem.gem2sam(mapped, index=gem_index_path, threads=nthreads,
                          single_end=single_end)
        _ = gem.sam2bam(sam, output=local_out_sam, threads=nthreads)
    else:
        sam = gem.gem2sam(mapped, index=gem_index_path, output=local_out_sam,
                          threads=nthreads, single_end=single_end)

    # Recursively go to the next iteration.
    unmapped_fastq_path = os.path.split(fastq_path)[1]
    if unmapped_fastq_path[-1].isdigit():
        unmapped_fastq_path = unmapped_fastq_path.rsplit('.', 1)[0]
    unmapped_fastq_path = os.path.join(
        temp_dir, unmapped_fastq_path + '.%d:%d-%d' % (
            N_WINDOWS - len(range_stop), seq_beg, seq_end))
    _filter_unmapped_fastq(fastq_path, local_out_sam, unmapped_fastq_path)

    out_files.extend(iterative_mapping(gem_index_path, unmapped_fastq_path,
                                       out_sam_path,
                                       range_start, range_stop, **kwargs))
    os.remove(unmapped_fastq_path)
    return out_files

def _line_count(path):
    '''Count the number of lines in a file. The function was posted by
    Mikola Kharechko on Stackoverflow.
    '''

    f = _gzopen(path)
    lines = 0
    buf_size = 1024 * 1024
    read_f = f.read  # loop optimization

    buf = read_f(buf_size)
    while buf:
        lines += buf.count('\n')
        buf = read_f(buf_size)

    return lines

def _chunk_file(in_path, out_basename, max_num_lines):
    '''Slice lines from a large file.
    The line numbering is as in Python slicing notation.
    '''
    num_lines = _line_count(in_path)
    if num_lines <= max_num_lines:
        return [in_path, ]

    out_paths = []

    for i, line in enumerate(_gzopen(in_path)):
        if i % max_num_lines == 0:
            out_path = out_basename + '.%d' % (i // max_num_lines + 1)
            out_paths.append(out_path)
            out_file = file(out_path, 'w')
        out_file.write(line)

    return out_paths

def _filter_fastq(ids, in_fastq, out_fastq):
    '''Filter FASTQ sequences by their IDs.

    Read entries from **in_fastq** and store in **out_fastq** only those
    the whose ID are in **ids**.
    '''
    out_file = open(out_fastq, 'w')
    in_file = _gzopen(in_fastq)
    while True:
        line = in_file.readline()
        if not line:
            break

        if not line.startswith('@'):
            raise Exception(
                '{0} does not comply with the FASTQ standards.'.format(in_fastq))

        fastq_entry = [line, in_file.readline(),
                       in_file.readline(), in_file.readline()]
        read_id = line.split()[0][1:]
        if read_id.endswith('/1') or read_id.endswith('/2'):
            read_id = read_id[:-2]
        if read_id in ids:
            out_file.writelines(fastq_entry)


def _filter_unmapped_fastq(in_fastq, in_sam, nonunique_fastq):
    '''Read raw sequences from **in_fastq** and alignments from
    **in_sam** and save the non-uniquely aligned and unmapped sequences
    to **unique_sam**.
    '''
    samfile = pysam.Samfile(in_sam)

    nonunique_ids = set()
    for read in samfile:
        tags_dict = dict(read.tags)
        read_id = read.qname
        # If exists, the option 'XS' contains the score of the second
        # best alignment. Therefore, its presence means non-unique alignment.
        if 'XS' in tags_dict or read.is_unmapped or (
            'NH' in tags_dict and int(tags_dict['NH']) > 1):
            nonunique_ids.add(read_id)
            
        # UNMAPPED reads should be included 5% chance to be mapped
        # with larger fragments, so do not do this:
        # if 'XS' in tags_dict or (
        #     'NH' in tags_dict and int(tags_dict['NH']) > 1):
        #     nonunique_ids.add(read_id)

    _filter_fastq(nonunique_ids, in_fastq, nonunique_fastq)

def _gzopen(path):
    if path.endswith('.gz'):
        return gzip.open(path)
    else:
        return open(path)
